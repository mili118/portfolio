import Link from 'next/link'

export const metadata = {
  title: 'What if Better AI Inference Isn\'t About Better Models?',
  description: 'Research on semantic data engineering for AI inference on pharmaceutical drug discovery',
}

<div className="min-h-screen bg-neutral-0 dark:bg-neutral-950 text-neutral-900 dark:text-neutral-100 flex items-center justify-center">
  <div className="max-w-4xl w-full px-8 py-16">
    <Link href="/" className="text-sm text-neutral-600 dark:text-neutral-400 hover:text-neutral-900 dark:hover:text-neutral-100 mb-10 inline-block">
      ‚Üê back
    </Link>

    <article className="space-y-6 leading-loose text-base">
      <header className="mb-10">
        <h1 className="text-3xl font-normal mb-3 tracking-tight">Semantic Data Engineering</h1>
        <p className="text-neutral-500 dark:text-neutral-600">Research on semantic data engineering for inference on pharmaceutical drug discovery</p>
      </header>

      <h2 className="text-2xl font-semibold mt-10 mb-4">What is Semantic Data Engineering</h2>

      <p>
        At its core, semantic data engineering uses ontologies, representations of concepts and their relationships, to structure data in ways that make meaning explicit. Regular data tells you what things are, but semantic data tells you what things are, how they relate to each other, what they mean in context, and why those relationships matter.
      </p>

      <h2 className="text-2xl font-semibold mt-10 mb-4">A Potentially Problematic Approach</h2>

      <p>
        The AI / ML research community has been obsessed with architecture. We went from basic neural networks to convolutional neural networks (CNNs) for vision. Then recurrent neural networks (RNNs) for sequences. Then LSTMs to handle the memory problem. Then attention mechanisms. Then Transformers, which dominated everything. Now we're seeing Mamba and state space models promising to be the next big leap.
      </p>

      <p>
        The issue is that you can only squeeze so much performance out of architectural innovation. At some point, you're just rearranging the same fundamental building blocks. The model gets bigger, the training gets more expensive, and the improvements get smaller.
      </p>

      <h2 className="text-2xl font-semibold mt-10 mb-4">The Data Problem</h2>

      <p>
        Reasoning in models is not just enabled by the model architecture itself, but the underlying semantic structure in the data plays an important role in this process. When LLMs can answer complex questions, it's because the training data contains semantic relationships: concepts, hierarchies, analogies, and ontologies to map out relationships.
      </p>

      <p>
        But a lot of the times, the data fed into these models are semantically messy and lack an actual structural representation.
      </p>

      <p>
        In specialized domains like healthcare, finance, or scientific research, the data often lacks clarity:
      </p>

      <ul className="list-disc ml-6 space-y-2">
        <li>Different databases use different terms for the same concept</li>
        <li>Relationships between entities are implicit, not explicit</li>
        <li>Context that humans understand instinctively is absent</li>
      </ul>

      <p>
        A highly sophisticated architecture could be trained on this data, and it'll still struggle with inference because the semantic structure isn't there.
      </p>

      <h2 className="text-2xl font-semibold mt-10 mb-4">Our Solution</h2>

      <p>
        Instead of tweaking model architectures, we intend to engineer the data itself to be semantically rich. This is not data cleaning or preprocessing in the traditional sense, but about fundamentally restructuring data to encode meaning, relationships, and context in ways that AI models can actually leverage for reasoning.
      </p>

      <p>
        To accomplish this, we're introducing <strong>Semantics 2.0</strong>: ontologies specifically designed to enhance AI inference:
      </p>

      <ul className="list-disc ml-6 space-y-2">
        <li><strong>Machine native</strong>: Built for computational reasoning, not human browsing</li>
        <li><strong>Inference optimized</strong>: Structured to enable the kinds of logical connections AI models need</li>
        <li><strong>Scalable</strong>: Can handle the massive, heterogeneous datasets that modern AI requires</li>
        <li><strong>Context-aware</strong>: Encode domain knowledge that models can't learn from data alone</li>
      </ul>

      <p>
        If you give a model data that's been semantically enriched with domain knowledge, explicit relationships, and contextual information, the model doesn't have to work as hard to infer meaning because the semantic structure does most of the work.
      </p>

      <h2 className="text-2xl font-semibold mt-10 mb-4">Why Test on Pharmaceutical Research</h2>

      <p>
        We're testing this hypothesis specifically on pharmaceutical drug discovery for a couple of reasons:
      </p>

      <p>
        Drug discovery involves reasoning across wildly different types of data:
      </p>

      <ul className="list-disc ml-6 space-y-2">
        <li>Genomic sequences and protein structures</li>
        <li>Chemical compound libraries</li>
        <li>Clinical trial outcomes</li>
        <li>Adverse event reports</li>
        <li>Scientific literature (millions of papers)</li>
        <li>Real-world patient data</li>
      </ul>

      <p>
        Each of these exists in different formats, uses different terminologies, and captures different aspects of biology and medicine. An AI model trying to predict whether a drug will work for a specific disease has to connect:
      </p>

      <ul className="list-disc ml-6 space-y-2">
        <li>Genetic mutations -> protein dysfunction -> disease symptoms</li>
        <li>Chemical structure -> biological activity -> therapeutic effect</li>
        <li>Preclinical results -> clinical outcomes -> safety profiles</li>
      </ul>

      <p>
        Drug discovery also has clear success metrics:
      </p>

      <ul className="list-disc ml-6 space-y-2">
        <li>Can the model predict which drug candidates will actually work?</li>
        <li>Can it identify safety issues before expensive clinical trials?</li>
        <li>Can it generalize to rare diseases where training data is scarce?</li>
      </ul>

      <p>
        If semantic data engineering improves these outcomes, we've proven something fundamental about how data structure affects AI inference.
      </p>

      <h2 className="text-2xl font-semibold mt-10 mb-4">Why I Think This Is Meaningful</h2>

      <p>
        As a third year undergraduate student, I don't have the formal experience and expertise to make meaningful contributions to the development of these AI models, as this is something that typically requires many years of dedicated research and deep expertise in relevant coursework.
      </p>

      <p>
        But semantic data engineering is different. Structuring data, understanding domain relationships, building ontologies are all more accessible. You don't need a PhD in machine learning to contribute meaningfully. You need domain knowledge, careful thinking about relationships and context, and good data engineering practices.
      </p>

      <p>
        This is important because there's traditionally been so much hype around architecture: where everyone wants to build the next Transformer or some new groundbreaking architecture. But data engineering is seen as unglamorous infrastructure work.
      </p>

      <p>
        I think people may have it backwards. What if the most impactful contributions to AI progress right now aren't coming from fancier architectures, but from people who deeply understand their domain and can encode that understanding into data structure? Semantic data engineering opens up AI research to a much broader group of contributors. You don't need access to massive compute clusters or specialized ML expertise, but you need to understand your data and think carefully about how to represent meaning.
      </p>

      <hr className="my-8 border-neutral-200 dark:border-neutral-800" />

      <h3 className="text-xl font-medium mt-8 mb-4">More Info</h3>

      <ul className="list-none space-y-2">
        <li><a href="https://sparkai.network/" target="_blank" rel="noopener noreferrer" className="underline underline-offset-4 decoration-1">SPARK AI Consortium</a></li>
        <li><a href="https://www.sdsc.edu" target="_blank" rel="noopener noreferrer" className="underline underline-offset-4 decoration-1">UC San Diego Supercomputer Center</a></li>
      </ul>
    </article>
  </div>
</div>
